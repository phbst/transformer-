{
  "version": "1.0",
  "truncation": null,
  "padding": null,
  "added_tokens": [
    {
      "id": 0,
      "content": "<unk>",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": false,
      "special": true
    }
  ],
  "normalizer": {
    "type": "BertNormalizer",
    "clean_text": true,
    "handle_chinese_chars": true,
    "strip_accents": null,
    "lowercase": false
  },
  "pre_tokenizer": {
    "type": "BertPreTokenizer"
  },
  "post_processor": null,
  "decoder": {
    "type": "BPEDecoder",
    "suffix": "</w>"
  },
  "model": {
    "type": "BPE",
    "dropout": null,
    "unk_token": "<unk>",
    "continuing_subword_prefix": null,
    "end_of_word_suffix": "</w>",
    "fuse_unk": false,
    "byte_fallback": false,
    "ignore_merges": false,
    "vocab": {
      "<unk>": 0,
      ",": 1,
      "-": 2,
      ".": 3,
      "B": 4,
      "C": 5,
      "D": 6,
      "E": 7,
      "F": 8,
      "H": 9,
      "L": 10,
      "M": 11,
      "N": 12,
      "P": 13,
      "S": 14,
      "T": 15,
      "a": 16,
      "b": 17,
      "c": 18,
      "d": 19,
      "e": 20,
      "f": 21,
      "g": 22,
      "h": 23,
      "i": 24,
      "k": 25,
      "l": 26,
      "m": 27,
      "n": 28,
      "o": 29,
      "p": 30,
      "q": 31,
      "r": 32,
      "s": 33,
      "t": 34,
      "u": 35,
      "v": 36,
      "w": 37,
      "x": 38,
      "y": 39,
      "z": 40,
      "s</w>": 41,
      "w</w>": 42,
      ",</w>": 43,
      "d</w>": 44,
      "e</w>": 45,
      "n</w>": 46,
      "g</w>": 47,
      "l</w>": 48,
      "r</w>": 49,
      "y</w>": 50,
      "t</w>": 51,
      "f</w>": 52,
      "-</w>": 53,
      "h</w>": 54,
      "m</w>": 55,
      "E</w>": 56,
      "u</w>": 57,
      "o</w>": 58,
      ".</w>": 59,
      "P</w>": 60,
      "a</w>": 61,
      "en": 62,
      "in": 63,
      "ken": 64,
      "oken": 65,
      "an": 66,
      "iz": 67,
      "token": 68,
      "or": 69,
      "ti": 70,
      "er": 71,
      "th": 72,
      "on</w>": 73,
      "tokeniz": 74,
      "ar": 75,
      "is</w>": 76,
      "wor": 77,
      "tion</w>": 78,
      "ation</w>": 79,
      "ed</w>": 80,
      "ac": 81,
      "al": 82,
      "of</w>": 83,
      "te": 84,
      "ing</w>": 85,
      "tokenization</w>": 86,
      "Th": 87,
      "ch": 88,
      "ds</w>": 89,
      "ub": 90,
      "and</w>": 91,
      "ce</w>": 92,
      "el": 93,
      "es": 94,
      "it": 95,
      "od": 96,
      "ou": 97,
      "ra": 98,
      "sub": 99,
      "ers</w>": 100,
      "the</w>": 101,
      "word</w>": 102,
      "words</w>": 103,
      "ab": 104,
      "as": 105,
      "ec": 106,
      "es</w>": 107,
      "er</w>": 108,
      "ff": 109,
      "fi": 110,
      "gu": 111,
      "mod": 112,
      "oc": 113,
      "om": 114,
      "ow": 115,
      "or</w>": 116,
      "st": 117,
      "to</w>": 118,
      "ul": 119,
      "un": 120,
      "tokens</w>": 121,
      "arac": 122,
      "This</w>": 123,
      "charac": 124,
      "Token": 125,
      "ag": 126,
      "at": 127,
      "an</w>": 128,
      "al</w>": 129,
      "be</w>": 130,
      "bas": 131,
      "com": 132,
      "can</w>": 133,
      "gen": 134,
      "gor": 135,
      "il": 136,
      "ir</w>": 137,
      "ith": 138,
      "iff": 139,
      "le": 140,
      "le</w>": 141,
      "ly</w>": 142,
      "lan": 143,
      "me": 144,
      "op": 145,
      "pr": 146,
      "re": 147,
      "re</w>": 148,
      "se</w>": 149,
      "tra": 150,
      "ters</w>": 151,
      "ent</w>": 152,
      "ined</w>": 153,
      "ance</w>": 154,
      "erent</w>": 155,
      "the": 156,
      "tokenizer</w>": 157,
      "all": 158,
      "algor": 159,
      "ter</w>": 160,
      "els</w>": 161,
      "ese</w>": 162,
      "its</w>": 163,
      "out</w>": 164,
      "subword</w>": 165,
      "fic": 166,
      "guag": 167,
      "models</w>": 168,
      "oces": 169,
      "own</w>": 170,
      "units</w>": 171,
      "characters</w>": 172,
      "Tokeniz": 173,
      "based</w>": 174,
      "ifferent</w>": 175,
      "languag": 176,
      "proces": 177,
      "trained</w>": 178,
      "algorith": 179
    },
    "merges": [
      "e n",
      "i n",
      "k en",
      "o ken",
      "a n",
      "i z",
      "t oken",
      "o r",
      "t i",
      "e r",
      "t h",
      "o n</w>",
      "token iz",
      "a r",
      "i s</w>",
      "w or",
      "ti on</w>",
      "a tion</w>",
      "e d</w>",
      "a c",
      "a l",
      "o f</w>",
      "t e",
      "in g</w>",
      "tokeniz ation</w>",
      "T h",
      "c h",
      "d s</w>",
      "u b",
      "an d</w>",
      "c e</w>",
      "e l",
      "e s",
      "i t",
      "o d",
      "o u",
      "r a",
      "s ub",
      "er s</w>",
      "th e</w>",
      "wor d</w>",
      "wor ds</w>",
      "a b",
      "a s",
      "e c",
      "e s</w>",
      "e r</w>",
      "f f",
      "f i",
      "g u",
      "m od",
      "o c",
      "o m",
      "o w",
      "o r</w>",
      "s t",
      "t o</w>",
      "u l",
      "u n",
      "token s</w>",
      "ar ac",
      "Th is</w>",
      "ch arac",
      "T oken",
      "a g",
      "a t",
      "a n</w>",
      "a l</w>",
      "b e</w>",
      "b as",
      "c om",
      "c an</w>",
      "g en",
      "g or",
      "i l",
      "i r</w>",
      "i th",
      "i ff",
      "l e",
      "l e</w>",
      "l y</w>",
      "l an",
      "m e",
      "o p",
      "p r",
      "r e",
      "r e</w>",
      "s e</w>",
      "t ra",
      "t ers</w>",
      "en t</w>",
      "in ed</w>",
      "an ce</w>",
      "er ent</w>",
      "th e",
      "tokeniz er</w>",
      "al l",
      "al gor",
      "te r</w>",
      "el s</w>",
      "es e</w>",
      "it s</w>",
      "ou t</w>",
      "sub word</w>",
      "fi c",
      "gu ag",
      "mod els</w>",
      "oc es",
      "ow n</w>",
      "un its</w>",
      "charac ters</w>",
      "Token iz",
      "bas ed</w>",
      "iff erent</w>",
      "lan guag",
      "pr oces",
      "tra ined</w>",
      "algor ith"
    ]
  }
}