{
  "version": "1.0",
  "truncation": null,
  "padding": null,
  "added_tokens": [
    {
      "id": 0,
      "content": "<unk>",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": false,
      "special": true
    }
  ],
  "normalizer": {
    "type": "BertNormalizer",
    "clean_text": true,
    "handle_chinese_chars": true,
    "strip_accents": null,
    "lowercase": false
  },
  "pre_tokenizer": {
    "type": "BertPreTokenizer"
  },
  "post_processor": null,
  "decoder": {
    "type": "BPEDecoder",
    "suffix": "</w>"
  },
  "model": {
    "type": "BPE",
    "dropout": null,
    "unk_token": "<unk>",
    "continuing_subword_prefix": null,
    "end_of_word_suffix": "</w>",
    "fuse_unk": false,
    "byte_fallback": false,
    "ignore_merges": false,
    "vocab": {
      "<unk>": 0,
      ",": 1,
      ".": 2,
      "C": 3,
      "F": 4,
      "H": 5,
      "T": 6,
      "a": 7,
      "b": 8,
      "c": 9,
      "d": 10,
      "e": 11,
      "f": 12,
      "g": 13,
      "h": 14,
      "i": 15,
      "k": 16,
      "l": 17,
      "m": 18,
      "n": 19,
      "o": 20,
      "p": 21,
      "r": 22,
      "s": 23,
      "t": 24,
      "u": 25,
      "v": 26,
      "w": 27,
      "y": 28,
      "z": 29,
      "g</w>": 30,
      "d</w>": 31,
      "t</w>": 32,
      "s</w>": 33,
      "e</w>": 34,
      ".</w>": 35,
      "r</w>": 36,
      "y</w>": 37,
      "l</w>": 38,
      "u</w>": 39,
      "n</w>": 40,
      ",</w>": 41,
      "w</w>": 42,
      "o</w>": 43,
      "is</w>": 44,
      "en": 45,
      "Th": 46,
      "er": 47,
      "ken": 48,
      "oken": 49,
      "th": 50,
      "token": 51,
      "This</w>": 52,
      "ab": 53,
      "an": 54,
      "at": 55,
      "er</w>": 56,
      "ho": 57,
      "in": 58,
      "io": 59,
      "iz": 60,
      "ou": 61,
      "se": 62,
      "tokeniz": 63,
      "and</w>": 64,
      "ion</w>": 65
    },
    "merges": [
      "i s</w>",
      "e n",
      "T h",
      "e r",
      "k en",
      "o ken",
      "t h",
      "t oken",
      "Th is</w>",
      "a b",
      "a n",
      "a t",
      "e r</w>",
      "h o",
      "i n",
      "i o",
      "i z",
      "o u",
      "s e",
      "token iz",
      "an d</w>",
      "io n</w>"
    ]
  }
}